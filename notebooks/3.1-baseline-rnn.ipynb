{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f776180b-b6e5-437e-98fd-fcbda1aed8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "dir2 = os.path.abspath('')\n",
    "dir1 = os.path.dirname(dir2)\n",
    "if not dir1 in sys.path:\n",
    "    sys.path.append(dir1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d63acf8-234a-416f-b157-38e703a0c12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "from src.data.make_dataset import TextDetoxificationDataset, Evaluator\n",
    "\n",
    "from src.models.train_model import BaselineTranslationModel\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8df7bd71-336e-444f-b6b0-9a8e4fd2fbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Oct 21 17:41:18 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 536.67                 Driver Version: 536.67       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3050 ...  WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   80C    P0              35W /  35W |   3192MiB /  4096MiB |    100%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32c679cd-05ba-42c2-81df-0ac5ecaf7c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mirak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "\u001b[32m2023-10-21 17:41:21.836\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.data.make_dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m212\u001b[0m - \u001b[1mStarted building vocab\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fce7a5c634f4a7597c0cf750ed99069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting vocab: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-10-21 17:43:13.940\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.data.make_dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mVocab built successfully\u001b[0m\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mirak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TextDetoxificationDataset(mode='train')\n",
    "val_dataset = TextDetoxificationDataset(mode='val', vocab=train_dataset.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0331ce7-3c41-48bd-8464-2228bd79a7ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54a56a97b2f140939c56cc48db6362a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Initializing with pretrained embeddings:   0%|          | 0/20085 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mirak\\PycharmProjects\\TextDetoxification\\src\\models\\train_model.py:44: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:212.)\n",
      "  self.embeddings.weight.data[self.vocab[word]] = torch.Tensor(glove_model.get_vector(word))\n"
     ]
    }
   ],
   "source": [
    "model = BaselineTranslationModel(vocab=train_dataset.vocab, emb_dim=200, hidden_dim=200, n_layers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed81e7b8-6d3e-49e3-afc5-62ec14365cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65601796 0.19445348\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    w1 = model.embeddings(torch.Tensor([model.vocab['king']]).long()).numpy().squeeze()\n",
    "    w2 = model.embeddings(torch.Tensor([model.vocab['queen']]).long()).numpy().squeeze()\n",
    "    w3 = model.embeddings(torch.Tensor([model.vocab['man']]).long()).numpy().squeeze()\n",
    "    w4 = model.embeddings(torch.Tensor([model.vocab['woman']]).long()).numpy().squeeze()\n",
    "    w5 = model.embeddings(torch.Tensor([model.vocab['carrot']]).long()).numpy().squeeze()\n",
    "royal = w1 - w3\n",
    "queen2queen = (royal + w4).T @ w2 / np.linalg.norm(royal + w4) / np.linalg.norm(w2)\n",
    "queen2carrot = (royal + w4).T @ w5 / np.linalg.norm(royal + w4) / np.linalg.norm(w5)\n",
    "print(queen2queen, queen2carrot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae608616-56f7-4d7d-82a5-9729473575de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size: 145909920 bit | 18.24 MB\n"
     ]
    }
   ],
   "source": [
    "# code taken from https://discuss.pytorch.org/t/pytorch-model-size-in-mbs/149002\n",
    "size_model = 0\n",
    "for param in model.parameters():\n",
    "    if param.data.is_floating_point():\n",
    "        size_model += param.numel() * torch.finfo(param.data.dtype).bits\n",
    "    else:\n",
    "        size_model += param.numel() * torch.iinfo(param.data.dtype).bits\n",
    "print(f\"model size: {size_model} bit | {size_model / 8e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1dccdd-c5b7-456c-9fcf-f988baf88aa4",
   "metadata": {},
   "source": [
    "### DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "632a3356-e0ed-4de3-b065-39edbf9eec98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(logits_seq, out):\n",
    "    mask = out != model.vocab['<pad>'] # [batch_size, out_len]\n",
    "    targets_1hot = F.one_hot(out, len(model.vocab)).to(torch.float32)\n",
    "\n",
    "    # outputs of the model, [batch_size, out_len, num_tokens]\n",
    "    # logits_seq = model(inp, out, **inference_params)\n",
    "\n",
    "    # log-probabilities of all tokens at all steps, [batch_size, out_len, num_tokens]\n",
    "    logprobs_seq = torch.log(logits_seq.softmax(dim=-1) + 1e-6)\n",
    "\n",
    "    # log-probabilities of correct outputs, [batch_size, out_len]\n",
    "    logp_out = (logprobs_seq * targets_1hot).sum(dim=-1)\n",
    "\n",
    "    # average cross-entropy over non-padding tokens\n",
    "    return - torch.masked_select(logp_out, mask).mean() # average loss, scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77b472d5-75ff-403b-a232-8a2d7bc82489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch, max_len=64):\n",
    "    source, target = [], []\n",
    "    for src_sentence, tgt_sentence, _ in batch:\n",
    "        source.append(torch.Tensor([train_dataset.BOS_IDX] + src_sentence[:max_len].tolist() + [train_dataset.EOS_IDX]).long())\n",
    "        target.append(torch.Tensor([train_dataset.BOS_IDX] + tgt_sentence[:max_len].tolist() + [train_dataset.EOS_IDX]).long())\n",
    "    \n",
    "    source = torch.nn.utils.rnn.pad_sequence(source, batch_first=True, padding_value=train_dataset.PAD_IDX)\n",
    "    target = torch.nn.utils.rnn.pad_sequence(target, batch_first=True, padding_value=train_dataset.PAD_IDX)\n",
    "\n",
    "    return source.to(device), target.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f123c318-e35a-493e-90b0-0caef71f7717",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b66acb3-0405-41ac-8021-e5875c008478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82d021fc6458409285946388ee1a6f1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14445 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 45])\n",
      "torch.Size([32, 43])\n",
      "torch.Size([32, 43, 20085]) 20085\n",
      "11.783910751342773\n"
     ]
    }
   ],
   "source": [
    "# some santiy check\n",
    "with torch.no_grad():\n",
    "    model.to(device)\n",
    "    for src, tgt in tqdm.auto.tqdm(train_loader):\n",
    "        out = model(src, tgt)\n",
    "        print(src.shape)\n",
    "        print(tgt.shape)\n",
    "        print(out.shape, len(train_dataset.vocab))\n",
    "        loss = compute_loss(out, tgt)\n",
    "        print(loss.item())\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14520ed7-7c64-46dd-851f-f27a4b1d8822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.750530242919922\n",
      "1.4347864389419556\n"
     ]
    }
   ],
   "source": [
    "# Overfit a single batch\n",
    "batch = next(iter(train_loader))\n",
    "dummy_model = BaselineTranslationModel(vocab=train_dataset.vocab, emb_dim=100, hidden_dim=100, n_layers=1).to(device)\n",
    "optimizer = torch.optim.Adam(dummy_model.parameters(), lr=3e-4)\n",
    "loss_fn = compute_loss\n",
    "for i in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    out = dummy_model(src, tgt)\n",
    "    loss = compute_loss(out, tgt)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 200 == 0:\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0245527-bcd0-4c24-92e3-30475b0a6492",
   "metadata": {},
   "source": [
    "the model successfully overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33aa1a89-7aa8-42cf-adf7-bb9795588850",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2635490-a6cc-45ad-8707-739513941965",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "epochs = 5\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f973df-5d2e-4f54-8420-31a416426a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {'train_loss': [], 'dev_bleu': [], 'dev_loss': []}\n",
    "for _ in tqdm(range(epochs)):\n",
    "    for src, tgt in train_loader:\n",
    "        step = len(metrics['train_loss']) + 1\n",
    "        optimizer.zero_grad()\n",
    "        loss_fn = compute_loss(model(src), tgt)\n",
    "        loss_fn.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        metrics['train_loss'].append((step, loss_t.item()))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        metrics['dev_bleu'].append((step, bleu_score(model, dev_inp, dev_out)))\n",
    "\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(12,4))\n",
    "    for i, (name, history) in enumerate(sorted(metrics.items())):\n",
    "        plt.subplot(1, len(metrics), i + 1)\n",
    "        plt.title(name)\n",
    "        plt.plot(*zip(*history))\n",
    "        plt.grid()\n",
    "    plt.show()\n",
    "    print(\"Mean loss=%.3f\" % np.mean(metrics['train_loss'][-10:], axis=0)[1], flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08ccb6e-4529-4138-9b57-81cc5d112cf8",
   "metadata": {},
   "source": [
    "### Evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d59b591-6204-4070-ad7e-30c9dea75c5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
