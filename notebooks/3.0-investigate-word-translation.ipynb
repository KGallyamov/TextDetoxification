{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85dd3204-bc79-4b5b-aab6-2b0bcd5cb2b7",
   "metadata": {},
   "source": [
    "# Word-wise translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908831bd-bb1f-45af-8a8d-b83857a626d7",
   "metadata": {},
   "source": [
    "One approach to this is to isolate \"toxic\" vocabulary $X$ and \"anti-toxic\" vocabulary $Y$ and then find \n",
    "\n",
    "$W = \\arg\\min_W ||WX - Y||_2$\n",
    "\n",
    "reference paper: https://arxiv.org/pdf/1309.4168.pdf\n",
    "\n",
    "In other words, this is dictionary approach, but done in a more automatic way than constructing parallel words corpora by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ab90b0-f731-4b07-937b-305297d8b9d1",
   "metadata": {},
   "source": [
    "### Reducing parallel sentences to parallel words\n",
    "\n",
    "There is a [way](https://arxiv.org/pdf/1710.04087.pdf) to not use parallel corpora at all, but it is quite complicated for the baseline hence I don't implement it\n",
    "\n",
    "First idea: find sentences with high BLEU score, and compute their symmetric difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16d82b1c-3caf-4b04-8919-fbafc2265f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "dir2 = os.path.abspath('')\n",
    "dir1 = os.path.dirname(dir2)\n",
    "if not dir1 in sys.path:\n",
    "    sys.path.append(dir1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56e2da93-5562-4bd0-befc-24a918134fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from src.data.make_dataset import TextDetoxificationDataset, bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d302e56-6dd2-42de-9301-3644195c864d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mirak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb2eadfbc3e1420fad26c00a14cff48a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building vocab:   0%|          | 0/462221 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mirak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TextDetoxificationDataset(mode='train')\n",
    "val_dataset = TextDetoxificationDataset(mode='val', vocab=train_dataset.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "882024b0-87df-42ef-a222-d016a055c866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bbf01be13c5435290f51aebfc7ea091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/462221 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bleu_threshold = 0.80\n",
    "source_target = []\n",
    "bleus = []\n",
    "for src, tgt, stat in tqdm(train_dataset):\n",
    "    # take all sentences that \n",
    "    if src.shape != tgt.shape:\n",
    "        continue\n",
    "    src_n, tgt_n = src.numpy(), tgt.numpy()\n",
    "    bleu = bleu_score(src_n, tgt_n)\n",
    "    bleus.append(bleu)\n",
    "    if bleu > bleu_threshold:\n",
    "        source_target.extend(list(zip(src_n[src_n != tgt_n], tgt_n[src_n != tgt_n])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85853a57-e679-4f52-b8e8-8e6e3e516be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "433\n",
      "[('i', 'even'), ('farts', 'photos'), ('destroy', 'break'), ('kick-ass', 'buttercup'), ('cut', 'chop'), ('!', '?'), ('fuck', 'god'), ('burn', 'commit'), ('sex', 'balling'), ('electrocute', 'electrocuted'), ('spoiled', 'depraved'), ('to', 'we'), ('ugly', 'funny')]\n"
     ]
    }
   ],
   "source": [
    "print(len(source_target))\n",
    "source_target_tokens = [(train_dataset.idx2token[s], train_dataset.idx2token[t]) for s, t in source_target if s != 1 and t != 1]\n",
    "print(source_target_tokens[::30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d12346-81b8-4066-b4f6-be9727e6352c",
   "metadata": {},
   "source": [
    "### Train W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6645a15f-de4d-427c-ae9a-c101e740b5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "glove_model = api.load('glove-twitter-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a66a0315-0761-409b-80a4-cbc6142760f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([glove_model.get_vector(s) for s, t in source_target_tokens if s in glove_model and t in glove_model])\n",
    "Y_train = np.array([glove_model.get_vector(t) for s, t in source_target_tokens if s in glove_model and t in glove_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c459458b-7852-413c-b099-72c61949f65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVD is done for W's orthogonality\n",
    "U, S, Vh = np.linalg.svd(X_train.T @ Y_train, full_matrices=True)\n",
    "W = U @ Vh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed7cb388-ec7e-46c8-94d2-ae979d6284e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fart stick [('piss', 0.6232589483261108), ('freak', 0.5906803607940674), ('faggot', 0.5814687013626099), ('asshole', 0.5803543329238892)]\n",
      "booty quarry [('butt', 0.6418140530586243), ('fat', 0.6304983496665955), ('dick', 0.6218863725662231), ('vagina', 0.6089655160903931)]\n",
      "witch crone [('witches', 0.6530465483665466), ('witch', 0.6193137764930725), ('devil', 0.6115525364875793), ('fucker', 0.5908735990524292)]\n",
      "heart condition [('my', 0.7379490733146667), ('your', 0.7202531695365906), ('heart', 0.7148891091346741), ('like', 0.7004419565200806)]\n",
      "fuck get [('shit', 0.8758304715156555), ('fuck', 0.8559561967849731), ('stupid', 0.8324095010757446), ('damn', 0.8131234049797058)]\n",
      "rapist bully [('weirdo', 0.6210179924964905), ('pervert', 0.5540253520011902), ('wth', 0.5419331789016724), ('creep', 0.525635302066803)]\n",
      "cock pissing [('dick', 0.612026572227478), ('faggot', 0.6096116304397583), ('cunt', 0.6066538691520691), ('vagina', 0.6056788563728333)]\n",
      "beard whiskers [('beard', 0.6264841556549072), ('eye', 0.5751927495002747), ('horse', 0.5727846026420593), ('spider', 0.56260085105896)]\n",
      "thrown dropped [('thrown', 0.6769850850105286), ('kicked', 0.6542993187904358), ('onto', 0.6514111757278442), ('balls', 0.6394554376602173)]\n",
      "ignorant everything [('pathetic', 0.7903605103492737), ('ignorant', 0.7102258205413818), ('stupid', 0.7087119221687317), ('hypocritical', 0.6918317675590515)]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    source, target = source_target_tokens[np.random.randint(0, len(source_target_tokens))]\n",
    "    print(source, target, glove_model.most_similar([W @ glove_model[source]], topn=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6005bf9-0478-420c-b2b6-7f120d0baa68",
   "metadata": {},
   "source": [
    "# Conclusion / report\n",
    "\n",
    "The initial assumption under this approach (there is enough pairs where the key toxic word is changed for non-toxic one) does not seem to hold, so the best fit for the baseline would be simple recurrent encoder-decoder model \n",
    "\n",
    "It could also be the case that the performance would benefit from data selection, however, this solution will still be unlikely to best RNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
