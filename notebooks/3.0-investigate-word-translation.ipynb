{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85dd3204-bc79-4b5b-aab6-2b0bcd5cb2b7",
   "metadata": {},
   "source": [
    "# Word-wise translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908831bd-bb1f-45af-8a8d-b83857a626d7",
   "metadata": {},
   "source": [
    "One approach to this is to isolate \"toxic\" vocabulary $X$ and \"anti-toxic\" vocabulary $Y$ and then find \n",
    "\n",
    "$W = \\arg\\min_W ||WX - Y||_2$\n",
    "\n",
    "reference paper: https://arxiv.org/pdf/1309.4168.pdf\n",
    "\n",
    "In other words, this is dictionary approach, but done in a more automatic way than constructing parallel words corpora by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ab90b0-f731-4b07-937b-305297d8b9d1",
   "metadata": {},
   "source": [
    "### Reducing parallel sentences to parallel words\n",
    "\n",
    "There is a [way](https://arxiv.org/pdf/1710.04087.pdf) to not use parallel corpora at all, but it is quite complicated for the baseline hence I don't implement it\n",
    "\n",
    "First idea: find sentences with high BLEU score, and compute their symmetric difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16d82b1c-3caf-4b04-8919-fbafc2265f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "dir2 = os.path.abspath('')\n",
    "dir1 = os.path.dirname(dir2)\n",
    "if not dir1 in sys.path:\n",
    "    sys.path.append(dir1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56e2da93-5562-4bd0-befc-24a918134fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from src.data.make_dataset import TextDetoxificationDataset, Evaluator\n",
    "bleu_score = Evaluator.bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d302e56-6dd2-42de-9301-3644195c864d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mirak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "\u001b[32m2023-10-21 17:27:03.780\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.data.make_dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m212\u001b[0m - \u001b[1mStarted building vocab\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffe24630e6fc4e2c8191bbdeee5fce38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting vocab: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-10-21 17:29:33.522\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.data.make_dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m219\u001b[0m - \u001b[1mVocab built successfully\u001b[0m\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mirak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TextDetoxificationDataset(mode='train')\n",
    "val_dataset = TextDetoxificationDataset(mode='val', vocab=train_dataset.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "882024b0-87df-42ef-a222-d016a055c866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acf8cf6260f34811ab7d15edb1f6d726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/462221 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bleu_threshold = 0.80\n",
    "source_target = []\n",
    "bleus = []\n",
    "for src, tgt, stat in tqdm(train_dataset):\n",
    "    # take all sentences that \n",
    "    if src.shape != tgt.shape:\n",
    "        continue\n",
    "    src_n, tgt_n = src.numpy(), tgt.numpy()\n",
    "    bleu = bleu_score(src_n, tgt_n)\n",
    "    bleus.append(bleu)\n",
    "    if bleu > bleu_threshold:\n",
    "        source_target.extend(list(zip(src_n[src_n != tgt_n], tgt_n[src_n != tgt_n])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85853a57-e679-4f52-b8e8-8e6e3e516be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781\n",
      "[('i', 'even'), ('stupid', 'thick'), (\"n't\", 'have'), ('sex', 'sleeping'), ('cowards', 'celebrities'), ('because', \"'cause\"), ('.', '?'), ('fool', 'mutt'), ('shit', 'oh'), ('are', 'there'), ('shit', 'hell'), ('damn', 'no'), ('trying', 'to'), ('need', 'have'), ('.', '--'), ('nothing', 'something'), ('she', 'it'), ('shit', 'holy'), ('dick', 'douche'), ('you', '!'), ('containers', 'packaging'), ('shit', 'pad'), ('damn', 'jesus')]\n"
     ]
    }
   ],
   "source": [
    "print(len(source_target))\n",
    "source_target_tokens = [(train_dataset.vocab.get_itos()[s], train_dataset.vocab.get_itos()[t]) for s, t in source_target if s != 1 and t != 1]\n",
    "print(source_target_tokens[::30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d12346-81b8-4066-b4f6-be9727e6352c",
   "metadata": {},
   "source": [
    "### Train W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6645a15f-de4d-427c-ae9a-c101e740b5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "glove_model = api.load('glove-twitter-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a66a0315-0761-409b-80a4-cbc6142760f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([glove_model.get_vector(s) for s, t in source_target_tokens if s in glove_model and t in glove_model])\n",
    "Y_train = np.array([glove_model.get_vector(t) for s, t in source_target_tokens if s in glove_model and t in glove_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c459458b-7852-413c-b099-72c61949f65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVD is done for W's orthogonality\n",
    "U, S, Vh = np.linalg.svd(X_train.T @ Y_train, full_matrices=True)\n",
    "W = U @ Vh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed7cb388-ec7e-46c8-94d2-ae979d6284e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put get [('throw', 0.8304006457328796), ('put', 0.8286388516426086), ('get', 0.8241252303123474), ('take', 0.8208140134811401)]\n",
      "damn hell [('shit', 0.8791956305503845), ('damn', 0.8741188645362854), ('fuck', 0.8453155755996704), ('stupid', 0.8417540788650513)]\n",
      ". ? [('<repeat>', 0.7799089550971985), ('.', 0.7785171270370483), ('?', 0.7693502306938171), ('!', 0.7250091433525085)]\n",
      "cut crash [('cut', 0.7016690969467163), ('it', 0.70100998878479), ('put', 0.695357620716095), (\"'ll\", 0.6889913082122803)]\n",
      "crap thing [('crap', 0.8507955074310303), ('stupid', 0.8250228762626648), ('shit', 0.7968918681144714), ('stuff', 0.7954347729682922)]\n",
      "had were [('have', 0.8417242765426636), ('would', 0.8301723003387451), ('that', 0.8272920846939087), ('made', 0.826287567615509)]\n",
      "and to [('there', 0.8871590495109558), ('that', 0.8850626349449158), ('they', 0.868509829044342), ('if', 0.8650673031806946)]\n",
      "spoiled depraved [('pissed', 0.6166033744812012), ('cunt', 0.5962963700294495), ('hungover', 0.5896039009094238), ('bastard', 0.579298198223114)]\n",
      "whore harlot [('cunt', 0.6871077418327332), ('faggot', 0.6830821633338928), ('fucker', 0.670238733291626), ('stupid', 0.66344153881073)]\n",
      "men people [('like', 0.6265491843223572), ('jesus', 0.6259792447090149), ('them', 0.6085296869277954), ('all', 0.604069173336029)]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    source, target = source_target_tokens[np.random.randint(0, len(source_target_tokens))]\n",
    "    print(source, target, glove_model.most_similar([W @ glove_model[source]], topn=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6005bf9-0478-420c-b2b6-7f120d0baa68",
   "metadata": {},
   "source": [
    "# Conclusion / report\n",
    "\n",
    "The initial assumption under this approach (there is enough pairs where the key toxic word is changed for non-toxic one) does not seem to hold, so the best fit for the baseline would be simple recurrent encoder-decoder model \n",
    "\n",
    "It could also be the case that the performance would benefit from data selection, however, this solution will still be unlikely to best RNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
